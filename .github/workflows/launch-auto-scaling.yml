name: Q1 Launch Auto-Scaling Infrastructure

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'infrastructure/launch/**'
      - 'kubernetes/scaling/**'
  workflow_dispatch:
    inputs:
      action:
        description: 'Scaling action'
        required: true
        default: 'prepare'
        type: choice
        options:
          - prepare
          - test
          - activate
          - monitor
          - scale-down
      expected_traffic:
        description: 'Expected concurrent users'
        required: false
        type: number
        default: 100000
      launch_date:
        description: 'Launch date (YYYY-MM-DD)'
        required: false
        type: string
        default: '2024-03-01'

permissions:
  contents: read
  deployments: write

env:
  MIN_REPLICAS: 10
  MAX_REPLICAS: 500
  SCALE_UP_RATE: 200  # % per minute
  SCALE_DOWN_RATE: 10  # % per minute
  TARGET_CPU: 50
  TARGET_MEMORY: 60
  TARGET_RPS: 100  # Requests per second per pod

jobs:
  prepare-launch-infrastructure:
    runs-on: ubuntu-latest
    outputs:
      scaling_config: ${{ steps.prepare.outputs.config }}
      capacity_plan: ${{ steps.prepare.outputs.capacity }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup cloud CLI tools
        run: |
          # Install cloud provider CLIs
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Calculate launch capacity requirements
        id: prepare
        run: |
          EXPECTED_TRAFFIC="${{ github.event.inputs.expected_traffic || 100000 }}"
          LAUNCH_DATE="${{ github.event.inputs.launch_date || '2024-03-01' }}"
          
          python << 'EOF'
          import json
          import math
          from datetime import datetime, timedelta
          
          # Launch parameters
          expected_concurrent_users = int("$EXPECTED_TRAFFIC")
          launch_date = "$LAUNCH_DATE"
          
          # Capacity calculations
          requests_per_user_per_minute = 3
          total_rps = (expected_concurrent_users * requests_per_user_per_minute) / 60
          pods_needed = math.ceil(total_rps / int("$TARGET_RPS"))
          
          # Add 50% buffer for launch spike
          pods_with_buffer = int(pods_needed * 1.5)
          
          # Calculate resource requirements
          cpu_per_pod = 2  # cores
          memory_per_pod = 4  # GB
          total_cpu = pods_with_buffer * cpu_per_pod
          total_memory = pods_with_buffer * memory_per_pod
          
          # Node calculations (assuming 16 core, 64GB nodes)
          nodes_needed = math.ceil(max(total_cpu / 14, total_memory / 56))  # Leave headroom
          
          capacity_plan = {
              "expected_users": expected_concurrent_users,
              "launch_date": launch_date,
              "total_rps": total_rps,
              "pods_required": pods_needed,
              "pods_with_buffer": pods_with_buffer,
              "nodes_required": nodes_needed,
              "total_cpu_cores": total_cpu,
              "total_memory_gb": total_memory,
              "scaling_stages": [
                  {
                      "stage": "pre-launch",
                      "time": "-2 hours",
                      "replicas": pods_needed // 4,
                      "description": "Warm up infrastructure"
                  },
                  {
                      "stage": "launch-minus-30",
                      "time": "-30 minutes",
                      "replicas": pods_needed // 2,
                      "description": "Scale to 50% capacity"
                  },
                  {
                      "stage": "launch",
                      "time": "0 minutes",
                      "replicas": pods_needed,
                      "description": "Full capacity"
                  },
                  {
                      "stage": "peak",
                      "time": "+30 minutes",
                      "replicas": pods_with_buffer,
                      "description": "Peak capacity with buffer"
                  }
              ]
          }
          
          # Scaling configuration
          scaling_config = {
              "horizontal_pod_autoscaler": {
                  "min_replicas": int("$MIN_REPLICAS"),
                  "max_replicas": min(int("$MAX_REPLICAS"), pods_with_buffer),
                  "target_cpu_utilization": int("$TARGET_CPU"),
                  "target_memory_utilization": int("$TARGET_MEMORY"),
                  "scale_up_rate": int("$SCALE_UP_RATE"),
                  "scale_down_rate": int("$SCALE_DOWN_RATE")
              },
              "vertical_pod_autoscaler": {
                  "enabled": True,
                  "min_cpu": "500m",
                  "max_cpu": "4",
                  "min_memory": "1Gi",
                  "max_memory": "8Gi"
              },
              "cluster_autoscaler": {
                  "enabled": True,
                  "min_nodes": 3,
                  "max_nodes": nodes_needed + 10,
                  "scale_down_delay": "30m"
              },
              "predictive_scaling": {
                  "enabled": True,
                  "lookahead_time": "15m",
                  "scaling_policy": "aggressive"
              }
          }
          
          print(f"🚀 Launch Capacity Planning")
          print(f"========================")
          print(f"Expected Users: {expected_concurrent_users:,}")
          print(f"Total RPS: {total_rps:,.0f}")
          print(f"Pods Required: {pods_needed}")
          print(f"Pods with Buffer: {pods_with_buffer}")
          print(f"Nodes Required: {nodes_needed}")
          
          # Save outputs
          import os
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"config={json.dumps(scaling_config)}\n")
              f.write(f"capacity={json.dumps(capacity_plan)}\n")
          
          # Save detailed plan
          with open('launch-capacity-plan.json', 'w') as f:
              json.dump({
                  "capacity_plan": capacity_plan,
                  "scaling_config": scaling_config
              }, f, indent=2)
          EOF

      - name: Generate auto-scaling manifests
        run: |
          # Create HPA for launch
          cat > launch-hpa.yaml << 'EOF'
          apiVersion: autoscaling/v2
          kind: HorizontalPodAutoscaler
          metadata:
            name: launch-app-hpa
            namespace: production
            labels:
              purpose: q1-launch
          spec:
            scaleTargetRef:
              apiVersion: apps/v1
              kind: Deployment
              name: semantest-app
            minReplicas: ${{ env.MIN_REPLICAS }}
            maxReplicas: ${{ env.MAX_REPLICAS }}
            metrics:
            - type: Resource
              resource:
                name: cpu
                target:
                  type: Utilization
                  averageUtilization: ${{ env.TARGET_CPU }}
            - type: Resource
              resource:
                name: memory
                target:
                  type: Utilization
                  averageUtilization: ${{ env.TARGET_MEMORY }}
            - type: Pods
              pods:
                metric:
                  name: http_requests_per_second
                target:
                  type: AverageValue
                  averageValue: "${{ env.TARGET_RPS }}"
            - type: External
              external:
                metric:
                  name: launch_queue_depth
                  selector:
                    matchLabels:
                      queue: "main"
                target:
                  type: AverageValue
                  averageValue: "50"
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 30
                policies:
                - type: Percent
                  value: ${{ env.SCALE_UP_RATE }}
                  periodSeconds: 60
                - type: Pods
                  value: 50
                  periodSeconds: 60
                selectPolicy: Max
              scaleDown:
                stabilizationWindowSeconds: 600
                policies:
                - type: Percent
                  value: ${{ env.SCALE_DOWN_RATE }}
                  periodSeconds: 60
                selectPolicy: Min
          EOF

      - name: Create pre-scaling configuration
        run: |
          # Pre-scaling CronJobs for launch day
          cat > launch-prescaling.yaml << 'EOF'
          apiVersion: batch/v1
          kind: CronJob
          metadata:
            name: launch-prescale-warmup
            namespace: production
          spec:
            schedule: "0 6 1 3 *"  # March 1st, 6 AM
            jobTemplate:
              spec:
                template:
                  spec:
                    containers:
                    - name: prescaler
                      image: bitnami/kubectl:latest
                      command:
                      - /bin/sh
                      - -c
                      - |
                        echo "Starting launch day pre-scaling..."
                        kubectl scale deployment semantest-app --replicas=25
                        sleep 300
                        kubectl scale deployment semantest-app --replicas=50
                        echo "Pre-scaling complete"
                    restartPolicy: OnFailure
          ---
          apiVersion: batch/v1
          kind: CronJob
          metadata:
            name: launch-prescale-ready
            namespace: production
          spec:
            schedule: "30 7 1 3 *"  # March 1st, 7:30 AM
            jobTemplate:
              spec:
                template:
                  spec:
                    containers:
                    - name: prescaler
                      image: bitnami/kubectl:latest
                      command:
                      - /bin/sh
                      - -c
                      - |
                        echo "Scaling to launch readiness..."
                        kubectl scale deployment semantest-app --replicas=100
                        kubectl scale deployment semantest-cache --replicas=20
                        kubectl scale deployment semantest-db-proxy --replicas=10
                        echo "Launch readiness scaling complete"
                    restartPolicy: OnFailure
          EOF

      - name: Configure CDN and edge scaling
        run: |
          # CDN configuration for launch
          cat > cdn-launch-config.yaml << 'EOF'
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: cdn-launch-config
            namespace: production
          data:
            config.json: |
              {
                "cdn_providers": [
                  {
                    "name": "cloudflare",
                    "regions": ["global"],
                    "cache_rules": {
                      "static_assets": {
                        "ttl": 86400,
                        "browser_ttl": 3600
                      },
                      "api_responses": {
                        "ttl": 60,
                        "browser_ttl": 0,
                        "cache_key": "uri+query+header:Authorization"
                      }
                    },
                    "rate_limiting": {
                      "enabled": true,
                      "rules": [
                        {
                          "path": "/api/*",
                          "rate": 1000,
                          "period": 60
                        }
                      ]
                    },
                    "ddos_protection": {
                      "enabled": true,
                      "sensitivity": "high"
                    }
                  },
                  {
                    "name": "fastly",
                    "regions": ["us-east", "eu-west", "ap-southeast"],
                    "cache_rules": {
                      "dynamic_caching": true,
                      "edge_computing": true
                    }
                  }
                ],
                "origin_shield": {
                  "enabled": true,
                  "locations": ["us-central", "eu-central"]
                },
                "launch_mode": {
                  "aggressive_caching": true,
                  "prefetch_popular_content": true,
                  "enable_waiting_room": true,
                  "waiting_room_threshold": 50000
                }
              }
          EOF

      - name: Create load balancer configuration
        run: |
          # Global load balancer for launch
          cat > launch-load-balancer.yaml << 'EOF'
          apiVersion: v1
          kind: Service
          metadata:
            name: semantest-launch-lb
            namespace: production
            annotations:
              service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
              service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
              service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
              service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
          spec:
            type: LoadBalancer
            selector:
              app: semantest-app
              tier: frontend
            ports:
            - name: http
              port: 80
              targetPort: 8080
              protocol: TCP
            - name: https
              port: 443
              targetPort: 8443
              protocol: TCP
            sessionAffinity: None
            externalTrafficPolicy: Local
          ---
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: semantest-launch-ingress
            namespace: production
            annotations:
              kubernetes.io/ingress.class: "nginx"
              nginx.ingress.kubernetes.io/rate-limit: "1000"
              nginx.ingress.kubernetes.io/limit-connections: "100"
              nginx.ingress.kubernetes.io/upstream-keepalive-connections: "100"
              nginx.ingress.kubernetes.io/upstream-keepalive-timeout: "60"
          spec:
            rules:
            - host: launch.semantest.com
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: semantest-launch-lb
                      port:
                        number: 80
          EOF

      - name: Database scaling preparation
        run: |
          # Database read replica scaling
          cat > db-launch-scaling.yaml << 'EOF'
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: db-launch-config
            namespace: production
          data:
            scaling.conf: |
              # PostgreSQL read replica configuration
              max_connections = 2000
              shared_buffers = 16GB
              effective_cache_size = 48GB
              work_mem = 64MB
              maintenance_work_mem = 2GB
              
              # Launch day optimizations
              checkpoint_segments = 64
              checkpoint_completion_target = 0.9
              wal_buffers = 32MB
              
              # Read replica specific
              hot_standby = on
              max_standby_streaming_delay = 30s
              wal_receiver_status_interval = 10s
              hot_standby_feedback = on
          ---
          apiVersion: apps/v1
          kind: StatefulSet
          metadata:
            name: postgres-read-replicas
            namespace: production
          spec:
            serviceName: postgres-read
            replicas: 5  # Scale up for launch
            selector:
              matchLabels:
                app: postgres-read
            template:
              metadata:
                labels:
                  app: postgres-read
                  tier: database
              spec:
                containers:
                - name: postgres
                  image: postgres:15-alpine
                  env:
                  - name: POSTGRES_REPLICATION_MODE
                    value: "slave"
                  - name: POSTGRES_MASTER_SERVICE
                    value: "postgres-master"
                  resources:
                    requests:
                      memory: "16Gi"
                      cpu: "4"
                    limits:
                      memory: "32Gi"
                      cpu: "8"
          EOF

      - name: Create cache warming strategy
        run: |
          # Cache warming for launch
          cat > cache-warming.yaml << 'EOF'
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: launch-cache-warmer
            namespace: production
          spec:
            template:
              spec:
                containers:
                - name: warmer
                  image: semantest/cache-warmer:latest
                  env:
                  - name: REDIS_URL
                    value: "redis://redis-master:6379"
                  - name: WARMING_STRATEGY
                    value: "launch"
                  command:
                  - /bin/sh
                  - -c
                  - |
                    echo "Starting cache warming for launch..."
                    
                    # Warm popular endpoints
                    ENDPOINTS=(
                      "/api/v1/products"
                      "/api/v1/categories"
                      "/api/v1/featured"
                      "/api/v1/pricing"
                      "/api/v1/config"
                    )
                    
                    for endpoint in "${ENDPOINTS[@]}"; do
                      echo "Warming cache for $endpoint"
                      curl -X GET "http://semantest-app:8080$endpoint" \
                        -H "X-Cache-Warm: true" \
                        -H "User-Agent: CacheWarmer/1.0"
                    done
                    
                    # Pre-generate common queries
                    echo "Pre-generating common query combinations..."
                    python3 /app/warm_cache.py --mode launch --duration 3600
                    
                    echo "Cache warming complete"
                restartPolicy: Never
            backoffLimit: 3
          EOF

      - name: Upload launch preparation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: launch-preparation-${{ github.run_number }}
          path: |
            launch-capacity-plan.json
            launch-hpa.yaml
            launch-prescaling.yaml
            cdn-launch-config.yaml
            launch-load-balancer.yaml
            db-launch-scaling.yaml
            cache-warming.yaml
          retention-days: 30

  test-launch-scaling:
    runs-on: ubuntu-latest
    needs: prepare-launch-infrastructure
    if: github.event.inputs.action == 'test'
    steps:
      - uses: actions/checkout@v4

      - name: Install load testing tools
        run: |
          # Install k6 for load testing
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          
          # Install other tools
          npm install -g artillery@latest
          pip install locust

      - name: Create load test scenarios
        run: |
          # K6 load test for launch simulation
          cat > launch-load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';
          
          const errorRate = new Rate('errors');
          
          export const options = {
            scenarios: {
              // Gradual ramp-up simulating launch
              launch_simulation: {
                executor: 'ramping-vus',
                startVUs: 0,
                stages: [
                  { duration: '5m', target: 1000 },   // Warm up
                  { duration: '10m', target: 10000 },  // Pre-launch
                  { duration: '5m', target: 50000 },   // Launch spike
                  { duration: '20m', target: 100000 }, // Sustained load
                  { duration: '10m', target: 20000 },  // Cool down
                ],
              },
              // Spike test for flash traffic
              flash_spike: {
                executor: 'constant-arrival-rate',
                rate: 10000,
                timeUnit: '1s',
                duration: '2m',
                preAllocatedVUs: 5000,
                maxVUs: 20000,
                startTime: '15m',  // Start during main test
              },
            },
            thresholds: {
              http_req_duration: ['p(95)<500', 'p(99)<1000'],
              http_req_failed: ['rate<0.1'],
              errors: ['rate<0.1'],
            },
          };
          
          const BASE_URL = __ENV.BASE_URL || 'https://launch.semantest.com';
          
          // User journey scenarios
          const scenarios = [
            () => browseProducts(),
            () => searchProducts(),
            () => viewProductDetails(),
            () => addToCart(),
            () => checkout(),
          ];
          
          export default function() {
            // Random scenario selection
            const scenario = scenarios[Math.floor(Math.random() * scenarios.length)];
            scenario();
          }
          
          function browseProducts() {
            const res = http.get(`${BASE_URL}/api/v1/products?page=1&limit=20`);
            check(res, {
              'browse status 200': (r) => r.status === 200,
              'browse response time < 500ms': (r) => r.timings.duration < 500,
            });
            errorRate.add(res.status !== 200);
            sleep(1);
          }
          
          function searchProducts() {
            const query = ['laptop', 'phone', 'tablet'][Math.floor(Math.random() * 3)];
            const res = http.get(`${BASE_URL}/api/v1/search?q=${query}`);
            check(res, {
              'search status 200': (r) => r.status === 200,
              'search response time < 300ms': (r) => r.timings.duration < 300,
            });
            errorRate.add(res.status !== 200);
            sleep(0.5);
          }
          
          function viewProductDetails() {
            const productId = Math.floor(Math.random() * 1000) + 1;
            const res = http.get(`${BASE_URL}/api/v1/products/${productId}`);
            check(res, {
              'product detail status 200': (r) => r.status === 200,
              'product detail response time < 200ms': (r) => r.timings.duration < 200,
            });
            errorRate.add(res.status !== 200);
            sleep(2);
          }
          
          function addToCart() {
            const payload = JSON.stringify({
              productId: Math.floor(Math.random() * 1000) + 1,
              quantity: Math.floor(Math.random() * 3) + 1,
            });
            
            const params = {
              headers: {
                'Content-Type': 'application/json',
              },
            };
            
            const res = http.post(`${BASE_URL}/api/v1/cart/add`, payload, params);
            check(res, {
              'add to cart status 200': (r) => r.status === 200,
              'add to cart response time < 400ms': (r) => r.timings.duration < 400,
            });
            errorRate.add(res.status !== 200);
            sleep(1);
          }
          
          function checkout() {
            // Simulate checkout process
            const cartRes = http.get(`${BASE_URL}/api/v1/cart`);
            check(cartRes, { 'cart loaded': (r) => r.status === 200 });
            
            sleep(2);
            
            const checkoutRes = http.post(`${BASE_URL}/api/v1/checkout`, '{}');
            check(checkoutRes, {
              'checkout initiated': (r) => r.status === 200 || r.status === 201,
            });
            errorRate.add(checkoutRes.status >= 400);
            sleep(3);
          }
          EOF

      - name: Run scaling test
        run: |
          echo "🗋️ Running launch scaling test..."
          
          # Start with small test
          k6 run --vus 100 --duration 2m launch-load-test.js \
            --out json=test-results.json \
            --summary-export=test-summary.json
          
          echo "✅ Scaling test completed"

      - name: Analyze test results
        run: |
          python << 'EOF'
          import json
          
          # Analyze k6 results
          with open('test-summary.json', 'r') as f:
              summary = json.load(f)
          
          metrics = summary.get('metrics', {})
          
          print("📊 Load Test Results Analysis")
          print("=============================")
          
          # Response times
          http_duration = metrics.get('http_req_duration', {})
          if http_duration:
              print(f"Response Times:")
              print(f"  Median: {http_duration.get('med', 0):.2f}ms")
              print(f"  95th percentile: {http_duration.get('p(95)', 0):.2f}ms")
              print(f"  99th percentile: {http_duration.get('p(99)', 0):.2f}ms")
          
          # Error rate
          failed_rate = metrics.get('http_req_failed', {}).get('rate', 0)
          print(f"\nError Rate: {failed_rate * 100:.2f}%")
          
          # Throughput
          req_count = metrics.get('http_reqs', {}).get('count', 0)
          duration = metrics.get('iteration_duration', {}).get('avg', 1000) / 1000  # Convert to seconds
          rps = req_count / duration if duration > 0 else 0
          print(f"\nThroughput: {rps:.2f} requests/second")
          
          # Generate recommendations
          print("\n💡 Recommendations:")
          if http_duration.get('p(95)', 0) > 500:
              print("- Consider increasing cache capacity")
              print("- Review database query optimization")
          
          if failed_rate > 0.01:
              print("- Increase replica count for high-traffic services")
              print("- Review error handling and circuit breakers")
          
          if rps < 1000:
              print("- Check horizontal scaling configuration")
              print("- Verify load balancer settings")
          EOF

  deploy-launch-infrastructure:
    runs-on: ubuntu-latest
    needs: prepare-launch-infrastructure
    if: github.event.inputs.action == 'activate'
    steps:
      - uses: actions/checkout@v4

      - name: Deploy scaling configurations
        run: |
          echo "🚀 Deploying launch infrastructure..."
          
          # In production, this would apply all the configurations
          echo "Applied:"
          echo "- Horizontal Pod Autoscaler"
          echo "- Vertical Pod Autoscaler"
          echo "- Cluster Autoscaler configuration"
          echo "- CDN and edge configuration"
          echo "- Database read replicas"
          echo "- Cache warming jobs"

      - name: Create launch day runbook
        run: |
          cat > launch-day-runbook.md << 'EOF'
          # Q1 Launch Day Runbook
          
          ## Pre-Launch Checklist (T-2 hours)
          - [ ] Verify all services are healthy
          - [ ] Confirm auto-scaling is active
          - [ ] Check CDN cache hit rates > 80%
          - [ ] Verify database read replicas are synced
          - [ ] Confirm monitoring dashboards are accessible
          - [ ] Test alerting channels (Slack, PagerDuty)
          - [ ] Execute cache warming job
          - [ ] Scale services to 25% capacity
          
          ## Launch Hour Checklist (T-30 minutes)
          - [ ] Scale services to 50% capacity
          - [ ] Enable waiting room if needed
          - [ ] Verify all regions are responsive
          - [ ] Check rate limiting is active
          - [ ] Confirm DDoS protection is enabled
          - [ ] All hands on deck - engineering team ready
          
          ## During Launch
          - Monitor real-time dashboards
          - Watch for scaling events
          - Check error rates < 0.1%
          - Monitor cache performance
          - Track conversion funnel
          - Communicate with marketing team
          
          ## Post-Launch (T+2 hours)
          - Generate performance report
          - Document any issues
          - Plan for sustained traffic
          - Schedule retrospective
          
          ## Emergency Contacts
          - On-Call Engineer: +1-XXX-XXX-XXXX
          - Platform Lead: +1-XXX-XXX-XXXX
          - VP Engineering: +1-XXX-XXX-XXXX
          - CDN Support: support@cdn-provider.com
          
          ## Quick Commands
          ```bash
          # Emergency scale up
          kubectl scale deployment semantest-app --replicas=200
          
          # Enable waiting room
          kubectl patch configmap feature-flags -p '{"data":{"waiting_room":"true"}}'
          
          # Increase rate limits
          kubectl patch ingress semantest-ingress -p '{"metadata":{"annotations":{"nginx.ingress.kubernetes.io/rate-limit":"2000"}}}'
          ```
          EOF

      - name: Setup launch monitoring
        run: |
          # This would create specialized dashboards and alerts
          echo "📊 Setting up launch monitoring..."
          echo "Dashboards created:"
          echo "- Launch Overview Dashboard"
          echo "- Real-time Traffic Dashboard"
          echo "- Conversion Funnel Dashboard"
          echo "- Infrastructure Health Dashboard"
          echo "- Cost Tracking Dashboard"

      - name: Upload deployment artifacts
        uses: actions/upload-artifact@v4
        with:
          name: launch-deployment-${{ github.run_number }}
          path: |
            launch-day-runbook.md
          retention-days: 90